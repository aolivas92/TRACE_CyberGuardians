import asyncio
import aiohttp
from bs4 import BeautifulSoup
import time
from typing import List
import csv
import aiofiles


class WebScraper:
    """
    Asynchronous web scraper to extract text content from web pages,
    including logos, labels, and class titles.

    Attributes:
        urls (list): List of URLs to scrape.
        concurrency (int): Maximum number of concurrent connections.

    Methods:
        scrape_pages(self, filename: str="scraped_output.csv") -> None

        async _fetch_url(self, url: str) -> str

        _extract_text_content(self, html: str) -> str

        async _scrape_pages_async(self) -> List
    """

    def __init__(self, urls: List, concurrency: int = 5):
        """
        Initialize with list of URLs and optional concurrency limit.

        Args:
            urls (list): List of URLs to scrape.
            concurrency (int): Max number of parallel fetches.
        """
        self.urls = urls
        self.concurrency = concurrency

    def scrape_pages(self, filename: str = "scraped_output.csv") -> List[str] | None:
        """
        Public main method to run the async scraping from sync code.
        """
        data = asyncio.run(self._scrape_pages_async())

        # Save the results
        with open(filename, "w", newline="", encoding="utf-8") as csvfile:
            csv_writer = csv.writer(csvfile)
            # Header line
            csv_writer.writerow(["id", "content", "url"])
            csv_writer.writerows(data)
        print(f"[INFO] CSV file {filename} has been generated.")
        return data

    async def _fetch_url(self, url: str) -> str | None:
        """
        Fetch the HTML content of a single URL asynchronously.

        Args:
            url (str): The URL to fetch.

        Returns:
            str: The raw HTML text is successful, or an empty string if an error occurs.
        """
        # TODO: Update to fetch the URLs from the database.
        """
        try:
            async with aiofiles.open(url, 'r', encoding='utf-8') as f:
                return await f.read()
        except Exception as e:
            print(f"[ERROR] Could not fetch {url}")
            return ""
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    response.raise_for_status()
                    return await response.text()
        except aiohttp.ClientError as e:
            print(f"[ERROR] Could not fetch {url}: {e}")
            return None

    def _extract_text_content(self, html: str) -> str:
        """
        Parse the HTML and extract:
            - Text from <p>, <h1>, <h2>, <h3>, <span>
            - Logos: text from images (often used for logos).
            - Labels: text that is listed inside labels.
            - Class titles: any CSS class attributes from elements.

        Args:
            html (str): the HTML content to parse.

        Returns:
            str: A combined string of extracted text.
        """
        soup = BeautifulSoup(html, "html.parser")

        # Gather textual content from standard text tags
        text_parts = []
        for tag in soup.find_all(
            [
                "p",
                "h1",
                "h2",
                "h3",
                "span",
                "label",
                "div",
                "section",
                "main",
                "li",
                "ul",
            ]
        ):
            text_parts.append(tag.get_text(strip=True))

        # Gather "logo" text from <img alt="..">
        for img in soup.find_all("img"):
            alt_text = img.get("alt")
            if alt_text:
                text_parts.append(alt_text)

        # Gather aria-label or aria-labelledby attribues
        # aria-labelledby is an ID reference, so we do a basic attempt to extract it
        for element in soup.find_all(attrs={"aria-label": True}):
            text_parts.append(element.get("aria-label"))
        for element in soup.find_all(attrs={"aria-labelledby": True}):
            label_id = element.get("aria-labelledby")
            if label_id:
                ref = soup.find(id=label_id)
                if ref and ref.text:
                    text_parts.append(ref.text.strip())

        # Gather class attributes from all elements
        for el in soup.find_all(attrs={"class": True}):
            class_attr = el.get("class")
            # class_attr might be a list of classes; join them
            if class_attr and isinstance(class_attr, list):
                text_parts.append(" ".join(class_attr))

        # Merge everything into one string
        return " ".join(part for part in text_parts if part)

    async def _scrape_pages_async(self) -> List:
        """
        Asynchronoulsy scrape text content from all URLs in self.urls.

        Args:
            None

        Returns:
            list of tuples: [(id, content, url), ...]
        """
        results = []
        sem = asyncio.Semaphore(self.concurrency)

        async with aiohttp.ClientSession() as session:

            async def scrape_page(i, url):
                async with sem:
                    html = await self._fetch_url(url)
                    if html:
                        text_content = self._extract_text_content(html)
                    else:
                        text_content = ""
                    results.append((i, text_content, url))

            tasks = []
            for i, url in enumerate(self.urls, start=1):
                tasks.append(scrape_page(i, url))

            await asyncio.gather(*tasks)

        # Sort the final results by ID
        results.sort(key=lambda x: x[0])
        return results

        print(f"CSV file '{filename}' has been generated.")
